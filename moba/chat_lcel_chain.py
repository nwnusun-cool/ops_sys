import gradio as gr
import json
import time
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Generator
import requests
import logging
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OllamaClient:
    """ä¼˜åŒ–çš„Ollamaå®¢æˆ·ç«¯ç±»"""

    def __init__(self, host: str = "http://192.168.100.17:11434"):
        self.host = host.rstrip('/')
        self.session = requests.Session()
        self.session.timeout = 60  # å¢åŠ è¶…æ—¶æ—¶é—´

    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.session.get(f"{self.host}/api/tags", timeout=10)
            response.raise_for_status()
            return True
        except Exception as e:
            logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def get_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.host}/api/tags")
            response.raise_for_status()
            models = response.json().get("models", [])
            model_names = [model["name"] for model in models]
            logger.info(f"è·å–åˆ°æ¨¡å‹åˆ—è¡¨: {model_names}")
            return model_names if model_names else ["qwen2.5:7b", "llama3.2:3b"]
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["qwen2.5:7b", "llama3.2:3b"]  # é»˜è®¤æ¨¡å‹

    def chat_stream(self, model: str, messages: List[Dict], **kwargs) -> Generator[str, None, None]:
        """æµå¼èŠå¤©"""
        try:
            payload = {
                "model": model,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("num_predict", -1),
                }
            }

            logger.info(f"å‘é€æµå¼è¯·æ±‚åˆ°æ¨¡å‹ {model}")
            logger.debug(f"è¯·æ±‚è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)}")

            response = self.session.post(
                f"{self.host}/api/chat",
                json=payload,
                stream=True,
                timeout=180
            )
            response.raise_for_status()

            result = ""
            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line.decode('utf-8'))
                        if chunk.get("message", {}).get("content"):
                            content = chunk["message"]["content"]
                            result += content
                            yield result
                        elif chunk.get("done"):
                            break
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSONè§£æé”™è¯¯: {e}, è¡Œå†…å®¹: {line}")
                        continue

            if not result:
                yield "æŠ±æ­‰ï¼Œæ¨¡å‹æ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚"

        except requests.exceptions.Timeout:
            error_msg = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
            logger.error(error_msg)
            yield error_msg
        except requests.exceptions.ConnectionError:
            error_msg = f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ ({self.host})ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ"
            logger.error(error_msg)
            yield error_msg
        except Exception as e:
            error_msg = f"èŠå¤©è¯·æ±‚å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield error_msg

    def chat(self, model: str, messages: List[Dict], **kwargs) -> str:
        """éæµå¼èŠå¤©"""
        try:
            payload = {
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("num_predict", -1),
                }
            }

            logger.info(f"å‘é€éæµå¼è¯·æ±‚åˆ°æ¨¡å‹ {model}")
            logger.debug(f"è¯·æ±‚è½½è·: {json.dumps(payload, indent=2, ensure_ascii=False)}")

            response = self.session.post(
                f"{self.host}/api/chat",
                json=payload,
                timeout=180
            )
            response.raise_for_status()

            result = response.json()["message"]["content"]
            return result if result else "æŠ±æ­‰ï¼Œæ¨¡å‹æ²¡æœ‰è¿”å›ä»»ä½•å†…å®¹ã€‚"

        except requests.exceptions.Timeout:
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
        except requests.exceptions.ConnectionError:
            return f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ ({self.host})ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ"
        except Exception as e:
            error_msg = f"èŠå¤©è¯·æ±‚å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg


class TimerDisplay:
    """è®¡æ—¶å™¨æ˜¾ç¤ºç±»"""

    def __init__(self):
        self.start_time = None
        self.is_running = False

    def start(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        self.is_running = True

    def stop(self):
        """åœæ­¢è®¡æ—¶"""
        self.is_running = False

    def get_elapsed_time(self) -> str:
        """è·å–å·²ç”¨æ—¶é—´"""
        if not self.is_running or self.start_time is None:
            return "00:00"

        elapsed = time.time() - self.start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        return f"{minutes:02d}:{seconds:02d}"


class ChatManager:
    """èŠå¤©ç®¡ç†å™¨"""

    def __init__(self):
        self.client = OllamaClient()
        self.conversation_history = []
        self.timer = TimerDisplay()
        self.system_prompts = {
            "é»˜è®¤": "",
            "ç¼–ç¨‹åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œèƒ½å¤Ÿæä¾›æ¸…æ™°çš„ä»£ç ç¤ºä¾‹å’Œè§£é‡Šã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
            "å†™ä½œåŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹ï¼Œæ“…é•¿åˆ›ä½œã€ä¿®æ”¹å’Œä¼˜åŒ–å„ç§æ–‡æœ¬å†…å®¹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
            "å­¦ä¹ å¯¼å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å­¦ä¹ å¯¼å¸ˆï¼Œå–„äºç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
            "ç¿»è¯‘ä¸“å®¶": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®ç¿»è¯‘å¤šç§è¯­è¨€ï¼Œå¹¶ä¿æŒåŸæ–‡çš„è¯­è°ƒå’Œé£æ ¼ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
        }

    def format_messages(self, history: List, current_message: str, system_prompt: str = "") -> List[Dict]:
        """æ ¼å¼åŒ–æ¶ˆæ¯å†å²"""
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})

        # å¤„ç†å†å²æ¶ˆæ¯
        if history:
            for entry in history:
                if isinstance(entry, dict) and "role" in entry and "content" in entry:
                    # æ–°æ ¼å¼ï¼š{"role": "user/assistant", "content": "..."}
                    messages.append(entry)
                elif isinstance(entry, (list, tuple)) and len(entry) >= 2:
                    # æ—§æ ¼å¼ï¼š[user_msg, assistant_msg]
                    user_msg, assistant_msg = entry[0], entry[1]
                    if user_msg and user_msg.strip():
                        messages.append({"role": "user", "content": user_msg})
                    if assistant_msg and assistant_msg.strip():
                        messages.append({"role": "assistant", "content": assistant_msg})

        # æ·»åŠ å½“å‰æ¶ˆæ¯
        if current_message.strip():
            messages.append({"role": "user", "content": current_message})

        logger.info(f"æ ¼å¼åŒ–åçš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        return messages

    def chat_response_stream(self, message: str, history: List, model: str,
                             system_prompt: str, temperature: float,
                             max_tokens: int) -> Generator[Tuple[List, str], None, None]:
        """æµå¼èŠå¤©å“åº”"""
        if not message.strip():
            yield history, "â±ï¸ 00:00"
            return

        # å¼€å§‹è®¡æ—¶
        self.timer.start()

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        new_history = history + [{"role": "user", "content": message}]

        # æ·»åŠ ç©ºçš„åŠ©æ‰‹å›å¤
        new_history.append({"role": "assistant", "content": ""})

        try:
            messages = self.format_messages(history, message, system_prompt)

            chat_params = {
                "temperature": temperature,
                "num_predict": max_tokens if max_tokens > 0 else -1
            }

            # å¼€å§‹æµå¼å“åº”
            for partial_response in self.client.chat_stream(model, messages, **chat_params):
                new_history[-1]["content"] = partial_response
                elapsed_time = self.timer.get_elapsed_time()
                yield new_history, f"â±ï¸ {elapsed_time}"

        except Exception as e:
            error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            new_history[-1]["content"] = error_msg
            yield new_history, f"â±ï¸ {self.timer.get_elapsed_time()}"
        finally:
            self.timer.stop()
            final_time = self.timer.get_elapsed_time()
            yield new_history, f"âœ… å®Œæˆ ({final_time})"

    def chat_response_non_stream(self, message: str, history: List, model: str,
                                 system_prompt: str, temperature: float,
                                 max_tokens: int) -> Tuple[List, str]:
        """éæµå¼èŠå¤©å“åº”"""
        if not message.strip():
            return history, "â±ï¸ 00:00"

        # å¼€å§‹è®¡æ—¶
        self.timer.start()

        try:
            messages = self.format_messages(history, message, system_prompt)

            chat_params = {
                "temperature": temperature,
                "num_predict": max_tokens if max_tokens > 0 else -1
            }

            response = self.client.chat(model, messages, **chat_params)

            # æ·»åŠ æ¶ˆæ¯åˆ°å†å²
            new_history = history + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": response}
            ]

            return new_history, f"âœ… å®Œæˆ ({self.timer.get_elapsed_time()})"

        except Exception as e:
            error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg)
            new_history = history + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": error_msg}
            ]
            return new_history, f"âŒ é”™è¯¯ ({self.timer.get_elapsed_time()})"
        finally:
            self.timer.stop()

    def export_conversation(self, history: List) -> str:
        """å¯¼å‡ºå¯¹è¯å†å²"""
        export_data = {
            "timestamp": datetime.now().isoformat(),
            "conversation": []
        }

        for entry in history:
            if isinstance(entry, dict):
                export_data["conversation"].append(entry)
            elif isinstance(entry, (list, tuple)) and len(entry) >= 2:
                export_data["conversation"].extend([
                    {"role": "user", "content": entry[0]},
                    {"role": "assistant", "content": entry[1]}
                ])

        return json.dumps(export_data, ensure_ascii=False, indent=2)


def create_chat_interface():
    """åˆ›å»ºèŠå¤©ç•Œé¢"""
    chat_manager = ChatManager()

    # æµ‹è¯•è¿æ¥å¹¶è·å–å¯ç”¨æ¨¡å‹
    if chat_manager.client.test_connection():
        available_models = chat_manager.client.get_models()
        connection_status = "âœ… Ollamaè¿æ¥æ­£å¸¸"
    else:
        available_models = ["qwen2.5:7b", "llama3.2:3b"]
        connection_status = "âŒ Ollamaè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ"

    with gr.Blocks(
            title="Enhanced Ollama Chat",
            theme=gr.themes.Soft(),
            css="""
        .chat-container { max-width: 1200px; margin: 0 auto; }
        .settings-panel { background: #f8f9fa; padding: 15px; border-radius: 10px; }
        .status-panel { background: #e8f5e8; padding: 10px; border-radius: 5px; margin: 10px 0; }
        .timer-display { font-family: monospace; font-weight: bold; color: #2196F3; }
        """
    ) as demo:

        gr.Markdown("# ğŸ¤– Enhanced Ollama Chat Assistant")
        gr.Markdown("åŠŸèƒ½å®Œå¤‡çš„æœ¬åœ°AIèŠå¤©ç•Œé¢ï¼Œæ”¯æŒå¤šæ¨¡å‹ã€æµå¼è¾“å‡ºã€è®¡æ—¶å™¨ç­‰åŠŸèƒ½")

        # è¿æ¥çŠ¶æ€æ˜¾ç¤º
        status_display = gr.Markdown(connection_status, elem_classes=["status-panel"])

        with gr.Row():
            with gr.Column(scale=4):
                # ä¸»èŠå¤©åŒºåŸŸ
                chatbot = gr.Chatbot(
                    height=600,
                    type="messages",
                    show_label=False,
                    show_copy_button=True,
                    show_share_button=False
                )

                # è®¡æ—¶å™¨æ˜¾ç¤º
                timer_display = gr.Markdown("â±ï¸ 00:00", elem_classes=["timer-display"])

                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        container=False,
                        scale=4,
                        autofocus=True
                    )
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)

                # å¿«æ·æŒ‰é’®
                with gr.Row():
                    clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", size="sm")
                    export_btn = gr.Button("å¯¼å‡ºå¯¹è¯", size="sm")
                    regen_btn = gr.Button("é‡æ–°ç”Ÿæˆ", size="sm")

            with gr.Column(scale=1, elem_classes=["settings-panel"]):
                gr.Markdown("### âš™ï¸ è®¾ç½®")

                # æ¨¡å‹é€‰æ‹©
                model_dropdown = gr.Dropdown(
                    choices=available_models,
                    value=available_models[0] if available_models else "qwen2.5:7b",
                    label="æ¨¡å‹",
                    info="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"
                )

                # è§’è‰²è®¾å®š
                system_dropdown = gr.Dropdown(
                    choices=list(chat_manager.system_prompts.keys()),
                    value="é»˜è®¤",
                    label="è§’è‰²è®¾å®š",
                    info="é€‰æ‹©AIçš„è§’è‰²"
                )

                # è‡ªå®šä¹‰ç³»ç»Ÿæç¤º
                custom_system = gr.Textbox(
                    label="è‡ªå®šä¹‰æç¤º",
                    placeholder="å¯é€‰ï¼šè¾“å…¥è‡ªå®šä¹‰ç³»ç»Ÿæç¤º",
                    lines=3
                )

                # é«˜çº§å‚æ•°
                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature",
                        info="æ§åˆ¶å›ç­”çš„éšæœºæ€§"
                    )

                    max_tokens = gr.Slider(
                        minimum=0,
                        maximum=4000,
                        value=0,
                        step=100,
                        label="æœ€å¤§ä»¤ç‰Œæ•°",
                        info="0è¡¨ç¤ºæ— é™åˆ¶"
                    )

                    stream_output = gr.Checkbox(
                        value=True,
                        label="æµå¼è¾“å‡º",
                        info="å®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹"
                    )

                # é¢„è®¾é—®é¢˜
                gr.Markdown("### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
                example_questions = [
                    "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                    "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº",
                    "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
                    "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ"
                ]

                for question in example_questions:
                    gr.Button(question, size="sm").click(
                        lambda q=question: q,
                        outputs=msg_input
                    )

        def get_system_prompt(system_choice, custom_prompt):
            """è·å–ç³»ç»Ÿæç¤º"""
            if custom_prompt.strip():
                return custom_prompt
            return chat_manager.system_prompts.get(system_choice, "")

        def respond(message, history, model, system_choice, custom_prompt,
                    temp, max_tok, stream):
            """å“åº”ç”¨æˆ·æ¶ˆæ¯"""
            if not message.strip():
                return history, "", "â±ï¸ 00:00"

            system_prompt = get_system_prompt(system_choice, custom_prompt)

            if stream:
                # æµå¼å“åº”
                for new_history, timer_text in chat_manager.chat_response_stream(
                        message, history, model, system_prompt, temp, max_tok
                ):
                    yield new_history, "", timer_text
            else:
                # éæµå¼å“åº”
                new_history, timer_text = chat_manager.chat_response_non_stream(
                    message, history, model, system_prompt, temp, max_tok
                )
                yield new_history, "", timer_text

        def clear_conversation():
            """æ¸…ç©ºå¯¹è¯"""
            return [], "", "â±ï¸ 00:00"

        def export_conversation(history):
            """å¯¼å‡ºå¯¹è¯"""
            if not history:
                return None, "æ²¡æœ‰å¯¹è¯è®°å½•å¯å¯¼å‡º"

            try:
                export_content = chat_manager.export_conversation(history)
                filename = f"chat_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(export_content)

                return filename, f"å¯¹è¯å·²å¯¼å‡ºåˆ° {filename}"
            except Exception as e:
                return None, f"å¯¼å‡ºå¤±è´¥: {str(e)}"

        def regenerate_response(history, model, system_choice, custom_prompt,
                                temp, max_tok, stream):
            """é‡æ–°ç”Ÿæˆæœ€åä¸€ä¸ªå›ç­”"""
            if not history or len(history) < 2:
                return history, "â±ï¸ 00:00"

            # ç§»é™¤æœ€åä¸€ä¸ªåŠ©æ‰‹å›ç­”
            if history[-1].get("role") == "assistant":
                last_user_message = None
                # æ‰¾åˆ°æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
                for i in range(len(history) - 2, -1, -1):
                    if history[i].get("role") == "user":
                        last_user_message = history[i]["content"]
                        break

                if last_user_message:
                    # ç§»é™¤æœ€åä¸€ä¸ªåŠ©æ‰‹å›ç­”
                    new_history = history[:-1]
                    system_prompt = get_system_prompt(system_choice, custom_prompt)

                    if stream:
                        for updated_history, timer_text in chat_manager.chat_response_stream(
                                last_user_message, new_history[:-1], model, system_prompt, temp, max_tok
                        ):
                            yield updated_history, timer_text
                    else:
                        updated_history, timer_text = chat_manager.chat_response_non_stream(
                            last_user_message, new_history[:-1], model, system_prompt, temp, max_tok
                        )
                        yield updated_history, timer_text
                else:
                    yield history, "â±ï¸ 00:00"
            else:
                yield history, "â±ï¸ 00:00"

        # äº‹ä»¶ç»‘å®š
        msg_input.submit(
            respond,
            inputs=[msg_input, chatbot, model_dropdown, system_dropdown,
                    custom_system, temperature, max_tokens, stream_output],
            outputs=[chatbot, msg_input, timer_display]
        )

        send_btn.click(
            respond,
            inputs=[msg_input, chatbot, model_dropdown, system_dropdown,
                    custom_system, temperature, max_tokens, stream_output],
            outputs=[chatbot, msg_input, timer_display]
        )

        clear_btn.click(
            clear_conversation,
            outputs=[chatbot, msg_input, timer_display]
        )

        export_btn.click(
            export_conversation,
            inputs=[chatbot],
            outputs=[gr.File(), gr.Textbox(visible=False)]
        )

        regen_btn.click(
            regenerate_response,
            inputs=[chatbot, model_dropdown, system_dropdown, custom_system,
                    temperature, max_tokens, stream_output],
            outputs=[chatbot, timer_display]
        )

    return demo


if __name__ == "__main__":
    import socket


    def find_free_port(start_port=7860, max_attempts=10):
        """æ‰¾åˆ°å¯ç”¨çš„ç«¯å£"""
        for port in range(start_port, start_port + max_attempts):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('localhost', port))
                    return port
            except OSError:
                continue
        return None


    # åˆ›å»ºå’Œå¯åŠ¨åº”ç”¨
    demo = create_chat_interface()

    # æ‰¾åˆ°å¯ç”¨ç«¯å£
    free_port = find_free_port()
    if free_port is None:
        print("æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ï¼Œä½¿ç”¨éšæœºç«¯å£")
        free_port = 0  # è®©ç³»ç»Ÿåˆ†é…éšæœºç«¯å£

    print(f"æ­£åœ¨å¯åŠ¨åº”ç”¨ï¼Œç«¯å£: {free_port}")

    # å¯åŠ¨é…ç½®
    try:
        demo.launch(
            server_name="127.0.0.1",
            server_port=free_port,
            share=False,
            debug=True,  # å¼€å¯è°ƒè¯•æ¨¡å¼ä¾¿äºæ’æŸ¥é—®é¢˜
            show_error=True,
            inbrowser=True,
            quiet=False  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨é…ç½®: {e}")
        # å¤‡ç”¨å¯åŠ¨é…ç½®
        demo.launch(
            server_name="127.0.0.1",
            server_port=0,
            share=False,
            debug=True,
            show_error=True,
            inbrowser=False,
            quiet=False
        )